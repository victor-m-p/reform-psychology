---
title: "test log-transformation"
author: "Victor MÃ¸ller Poulsen"
output:
  pdf_document: default
  html_document: default
---

# setup

```{r setup, include=FALSE}
# consider pacman
install.packages("pacman") 
library(pacman)

p_load(tidyverse, brms, ggthemes, bayesplot, cowplot, tidybayes, modelr)

# set up cmdstanr 
install.packages("cmdstanr", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
library(cmdstanr)
install_cmdstan(cores = 2, overwrite = TRUE)

setwd("/work/50114/MAG/modeling/code")
source("fun_helper.R")
```

# data

```{r}

d <- read_csv("/work/50114/MAG/data/modeling/psych_replication_matched.csv") %>%
  mutate(log_teamsize = log(n_authors), 
         condition_coded = ifelse(condition == "experiment", 1, 0),
         condition_fct = as_factor(condition), 
         teamsize_scaled = (n_authors-min(n_authors))/(max(n_authors)-min(n_authors)),
         days_after_2010_scaled = days_after_2010/max(days_after_2010),
         teamsize_log = log(n_authors),
         id_match = as_factor(match_group), 
         id_fct = as_factor(PaperId)) %>% # because min = 0
  glimpse()

```

# model formulae
same model-specifications as test_interactions2.
just differs by being log of teamsize rather than 0-1 scaling. 

```{r}

f_logteam_0 <- bf(c_5 ~ 0 + condition_fct + condition_fct:teamsize_log + (1|id_match))
f_logteam_1 <- bf(c_5 ~ 1 + condition_fct + condition_fct:teamsize_log + (1|id_match))
f_logteam_01 <- bf(c_5 ~ 0 + Intercept + condition_fct + condition_fct:teamsize_log + (1|id_match))

```

Just doing negbinomial() for now, since we had Rhat issues for both negative binomial and
zero-inflated negative binomial (does not seem to be the main cause of issues). 

f_team_0: b, sd, shape
f_team_1: b, Intercept, sd, shape
f_team_01: b, sd, shape (Intercept becomes b). 

set priors

```{r}

# negbin baseline
negbin_0 <- c(prior(gamma(0.01, 0.01), class = shape), 
              prior(normal(0, 1), class = b),
              prior(normal(0, 1), class = sd)) # a wild guess

# zinegbin baseline
negbin_1 = c(prior(gamma(0.01, 0.01), class = shape), 
             prior(normal(0, 1), class = b),
             prior(normal(0, 1), class = Intercept),
             prior(normal(0, 1), class = sd)) # a wild guess

# can be used for all interactions (without thinking)
negbin_01 <- c(prior(gamma(0.01, 0.01), class = shape), 
              prior(normal(0, 1), class = b),
              prior(normal(0, 1), class = sd)) # a wild guess



```

# sample prior only 
Some warnings and divergences. 

```{r, include=FALSE}

## baseline 
negbin_prior_log_0 <- fit_model(
  family = negbinomial(), 
  formula = f_logteam_0,
  prior = negbin_0,
  sample_prior = "only",
  file = "/work/50114/MAG/modeling/models/negbin_prior_log_0"
)

## baseline 
negbin_prior_log_1 <- fit_model(
  family = negbinomial(), 
  formula = f_logteam_1,
  prior = negbin_1,
  sample_prior = "only",
  file = "/work/50114/MAG/modeling/models/negbin_prior_log_1"
)

## baseline 
negbin_prior_log_01 <- fit_model(
  family = negbinomial(), 
  formula = f_logteam_01,
  prior = negbin_01,
  sample_prior = "only",
  file = "/work/50114/MAG/modeling/models/negbin_prior_log_01"
)

```

# check priors

```{r}

prior_check <- function(model, ndraws, title, xmax){
  
  pp_check(model, 
           ndraws = ndraws) +
    labs(title = title) + 
    theme_minimal() + 
    xlim(0, xmax)
  
} 

```

```{r}

prior_check(negbin_prior_log_0, 100, "No Intercept (x cutoff: 25)", 25)
prior_check(negbin_prior_log_1, 100, "Intercept (x cutoff: 25)", 25)
prior_check(negbin_prior_log_01, 100, "0 + Intercept (x cutoff: 25)", 25)

```

# fit models 
more pareto k than in the non-log-transformed?
still fewest in the 0 + Intercept model. 

```{r}

## baseline 
negbin_post_log_0 <- fit_model(
  family = negbinomial(), 
  formula = f_logteam_0,
  prior = negbin_0,
  sample_prior = TRUE,
  file = "/work/50114/MAG/modeling/models/negbin_post_log_0"
)

## baseline 
negbin_post_log_1 <- fit_model(
  family = negbinomial(), 
  formula = f_logteam_1,
  prior = negbin_1,
  sample_prior = TRUE,
  file = "/work/50114/MAG/modeling/models/negbin_post_log_1"
)

## baseline 
negbin_post_log_01 <- fit_model(
  family = negbinomial(), 
  formula = f_logteam_01,
  prior = negbin_01,
  sample_prior = TRUE,
  file = "/work/50114/MAG/modeling/models/negbin_post_log_01"
)


```

# check traces
looks ok. 

```{r}

# some auto-correlation
# effect almost entirely in random effect
plot(negbin_post_log_0, N = 3)

```


```{r}

# some auto-correlation
# effect almost entirely in random effect
plot(negbin_post_log_1, N = 3)

```

```{r}

# same issues 
plot(negbin_post_log_01, N = 3)

```

# check estimates
Fewer effective samples for the log model than non-log (test_interactions2). 
Again, also fewer samples for the non-intercept model. 

```{r}

print(negbin_post_log_0) 
print(negbin_post_log_1) # by far the highest number of effective samples 
print(negbin_post_log_01) 

```

# plot implications

```{r}

y <- d$c_5
y_0 <- posterior_predict(negbin_post_log_0, draws = 500)
y_1 <- posterior_predict(negbin_post_log_1, draws = 500)
y_01 <- posterior_predict(negbin_post_log_01, draws = 500)

```

## no intercept 
looks pretty good. 
Does overestimate number of 1, and underestimate number of 2 & 3 a bit (always does) -- perhaps zero-inflated better?
a lot of uncertainty around 0 and 1 still. 

```{r}
ppc_dens_overlay(y, y_0[1:50, ]) + xlim(0, 50)
ppc_hist(y, y_0[1:5, ]) + xlim(-1, 50)
```

## regular intercept
Perhaps a bit worse (e.g. with the undershoot at around c_5 = 5). 

```{r}
ppc_dens_overlay(y, y_1[1:50, ]) + xlim(0, 50)
ppc_hist(y, y_1[1:5, ]) + xlim(-1, 50)
```

## 0 + Intercept
Looks more or less the same.

```{r}
ppc_dens_overlay(y, y_01[1:50, ]) + xlim(0, 50)
ppc_hist(y, y_01[1:5, ]) + xlim(-1, 50)
```

# pareto k issues 
https://bookdown.org/content/4857/monsters-and-mixtures.html

## no intercept 
Mainly studies that are (relatively) low teamsize and high citation.
Also, several control studies that are the same (c_5 = 77, teamsize_log = 0.693, condition = control). 
Really need to fix this in the preprocessing. 
But here, we basically get that all studies with many citations are influential...

```{r}

d %>% 
  mutate(k = negbin_post_log_0$criteria$loo$diagnostics$pareto_k) %>% 
  filter(k > .7) %>% 
  select(c_5, teamsize_log, condition_fct, id_match, k)

```

## 1 + ...
some of the same here, mainly studies with high c_5 and low teamsize. 

```{r}

d %>% 
  mutate(k = negbin_post_log_1$criteria$loo$diagnostics$pareto_k) %>% 
  filter(k > .7) %>% 
  select(c_5, teamsize_log, condition_fct, id_match, k)

```


## 0 + Intercept 
Seems to handle influential observations a bit better.
Still issue with the repeated value as above & some "experiment"-condition.
Primarily those with high c_5. 

```{r}

# two studies that are the same in control (issue to be resolved earlier in the pipeline).
# the outlier study (experiment) which is max in teamsize and also extremely high citation
# whereas the 
d %>% 
  mutate(k = negbin_post_log_01$criteria$loo$diagnostics$pareto_k) %>% 
  filter(k > .7) %>% 
  select(c_5, teamsize_log, condition_fct, id_match, k)

```

# Quick model comparison
Basically no difference, but appears to prefer the intercept models. 
Do we know why that is? 

```{r}

loo_compare(negbin_post_log_0,
            negbin_post_log_1,
            negbin_post_log_01)

loo_model_weights(negbin_post_log_0,
                  negbin_post_log_1,
                  negbin_post_log_01)
```



