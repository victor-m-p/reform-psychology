---
title: "Untitled"
author: "Victor MÃ¸ller Poulsen"
date: "2/24/2022"
output: html_document
---

# important article
https://solomonkurz.netlify.app/post/2021-07-12-got-overdispersion-try-observation-level-random-effects-with-the-poisson-lognormal-mixture/

- we might be doing double duty with negative binomial,
since overdispersion can be solved with both (a) negative binomial or
(b) with random effects structure and then poisson. we could
consider trying (b). 

```{r setup, include=FALSE}
# consider pacman
install.packages("pacman") 
library(pacman)

p_load(tidyverse, brms, ggthemes, bayesplot, cowplot, tidybayes, modelr)

# set up cmdstanr 
install.packages("cmdstanr", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
library(cmdstanr)
install_cmdstan(cores = 2, overwrite = TRUE)

setwd("/work/50114/MAG/modeling/code")
source("fun_helper.R")
```

# read data and create some variables
## (1) temsize log. 
## (2) match group as id 

```{r}

# check data
d <- read_csv("/work/50114/MAG/data/modeling/psych_replication_matched.csv") %>%
  mutate(log_teamsize = log(n_authors), 
         condition_fct = as_factor(condition), 
         id_match = as_factor(match_group),
         id_dct = as_factor(PaperId),
         year_after_2010 = Year - 2010) %>% 
  glimpse()

```

# model formula 

```{r}

f <- bf(c_5 ~ 0 + condition_fct + condition_fct:log_teamsize + condition_fct:year_after_2010 + (0 + condition_fct | id_match))

```

get prior:
* b: effects.
* cor: correlation of random effecs I think
* sd: match and both conditions.
* shape: overdispersion parameter for negative binomial.

```{r}

get_prior(
  formula = f,
  data = d, 
  family = negbinomial())

```

# how to set priors (help)

```{r}

# log(15) for mean beta seems reasonable. 
d %>% group_by(condition_fct) %>%
  summarize(mean = mean(c_5),
            median = median(c_5),
            sd = sd(c_5))

```

# define prior
## revisit priors, for now very vague & relying on baseline.
## exponential(1) often used for sd. 
## gamma(0.01, 0.01) is baseline. 
## lkj(2) is often used for cor. 
## normal(log(15), 1) should be on the low side (but is actually high)
## I thought that intercept would correspond to mean actually. 

```{r}

# based on a bit of reasoning and wide priors
p_weak <- c(prior(gamma(0.01, 0.01), class = shape), # overdispersion parameter
           prior(normal(log(15), 1), class = b, coef = "condition_fctcontrol"),
           prior(normal(log(15), 1), class = b, coef = "condition_fctexperiment"),
           prior(normal(0, 1), class = b),
           prior(exponential(1), class = sd),
           prior(lkj(2), class = cor)) # Solomon Kurz (less flat). 

# tighter priors
p_regularize <- c(prior(gamma(0.01, 0.01), class = shape), # overdispersion parameter
                 prior(normal(log(15), 0.3), class = b, coef = "condition_fctcontrol"),
                 prior(normal(log(15), 0.3), class = b, coef = "condition_fctexperiment"),
                 prior(normal(0, 0.3), class = b),
                 prior(exponential(1), class = sd),
                 prior(lkj(5), class = cor)) # Solomon Kurz (less flat). 

# informed by prior-posterior update
p_informed <- c(prior(exponential(0.5), class = shape), # overdispersion parameter
                prior(normal(log(10), 0.5), class = b, coef = "condition_fctcontrol"),
                prior(normal(log(10), 0.5), class = b, coef = "condition_fctexperiment"),
                prior(normal(0.5, 0.3), coef = "condition_fctcontrol:log_teamsize"),
                prior(normal(0.5, 0.3), coef = "condition_fctexperiment:log_teamsize"),
                prior(normal(0, 0.3), class = b),
                prior(exponential(1), class = sd),
                prior(lkj(5), class = cor)) # Solomon Kurz (less flat). 

```

# sample prior

```{r, include = FALSE}

# divergences (~ 1%)
m_prior_weak <- fit_model(
  family = negbinomial(), 
  formula = f,
  prior = p_weak,
  sample_prior = "only",
  file = "/work/50114/MAG/modeling/models/m_prior_weak"
)

# divergences (~ 1%)
m_prior_reg <- fit_model(
  family = negbinomial(), 
  formula = f, 
  prior = p_regularize,
  sample_prior = "only",
  file = "/work/50114/MAG/modeling/models/m_prior_regularize"
)

# no divergences. 
m_prior_inf <- fit_model(
  family = negbinomial(), 
  formula = f, 
  prior = p_informed,
  sample_prior = "only",
  file = "/work/50114/MAG/modeling/models/m_prior_informed"
)

```

# prior predictive check
## overall 

```{r}

# get up to many millions here, too crazy. 
pp_check(m_prior_weak, ndraws = 50)

```

```{r}

# can also go up to millions, but more like a couple of thousand. 
pp_check(m_prior_reg, ndraws = 50)

```

```{r}

# goes up to millions as well. 
pp_check(m_prior_inf, ndraws = 50)

```

## zoom
prior is consistent with almost all studies being = 0. 
that might be too crazy to allow in the prior.
the case for both priors here. 

```{r}

pp_check(m_prior_weak, ndraws = 50) + xlim(0, 50)

```

```{r}

pp_check(m_prior_reg, ndraws = 50) + xlim(0, 50)

```

```{r}

# looks much better than the other two models. 
pp_check(m_prior_inf, ndraws = 50) + xlim(0, 50)

```


# fit model
a lot of pareto k > 0.7 & p_waic > 0.4 

```{r}

m_post_weak <- fit_model(
  family = negbinomial(), 
  formula = f,
  prior = p_weak,
  sample_prior = TRUE,
  file = "/work/50114/MAG/modeling/models/m_post_weak"
)

m_post_reg <- fit_model(
  family = negbinomial(), 
  formula = f, 
  prior = p_regularize,
  sample_prior = TRUE,
  file = "/work/50114/MAG/modeling/models/m_post_reg"
)

m_post_inf <- fit_model(
  family = negbinomial(), 
  formula = f, 
  prior = p_informed,
  sample_prior = TRUE,
  file = "/work/50114/MAG/modeling/models/m_post_inf"
)

```

# check trace
overall ok, some issues with: 
* cor_id_match 
* sd_id_match 
better in "informed" model

```{r}

plot(m_post_weak, N = 3)

```

```{r}

plot(m_post_reg, N = 3)

```

```{r}

plot(m_post_inf, N = 3)

```

# posterior predictive check
## overall

```{r}

# up to ~4000-6000 typically
pp_check(m_post_weak, ndraws = 100)

```

```{r}

# up to 1500-2000 typically
pp_check(m_post_reg, ndraws = 100)

```

```{r}

# around 2000-3000 typically
pp_check(m_post_inf, ndraws = 100)

```

## zoom

```{r}

# overshoots zeros a bit + undershoots 15-ish. 
pp_check(m_post_weak, ndraws = 50) + xlim(0, 100)

```

```{r}

pp_check(m_post_reg, ndraws = 50) + xlim(0, 100)

```

```{r}

pp_check(m_post_inf, ndraws = 50) + xlim(0, 100)

```

# Updating 
## first for beta effects. 
## Weird that I do not have the prior in the posterior model?


```{r}

plot_update_beta <- function(m_post, m_prior){
  
  post_draws <- as_draws_df(m_post)
  prior_draws <- as_draws_df(m_prior)
  
  p1 <- prior_draws %>% ggplot() +
    geom_density(aes(b_condition_fctexperiment), fill= "steelblue", color = "black", alpha = 0.6) + 
    geom_density(aes(b_condition_fctexperiment), fill="#FC4E07", color="black",alpha=0.6, data = post_draws) + 
    theme_classic() + 
    labs(title = 'experiment intercept')
  
  p2 <- prior_draws %>% ggplot() + 
    geom_density(aes(b_condition_fctcontrol), fill= "steelblue", color = "black", alpha = 0.6) + 
    geom_density(aes(b_condition_fctcontrol), fill="#FC4E07", color="black",alpha=0.6, data = post_draws) + 
    theme_classic() + 
    labs(title = 'control intercept')
  
  p3 <- prior_draws %>% ggplot() + 
    geom_density(aes(`b_condition_fctexperiment:log_teamsize`), fill= "steelblue", color = "black", alpha = 0.6) + 
    geom_density(aes(`b_condition_fctexperiment:log_teamsize`), fill="#FC4E07", color="black",alpha=0.6, data = post_draws) + 
    theme_classic() + 
    labs(title = 'experiment interaction log(teamsize)')
  
  p4 <- prior_draws %>% ggplot() + 
    geom_density(aes(`b_condition_fctcontrol:log_teamsize`), fill= "steelblue", color = "black", alpha = 0.6) + 
    geom_density(aes(`b_condition_fctcontrol:log_teamsize`), fill="#FC4E07", color="black",alpha=0.6, data = post_draws) + 
    theme_classic() + 
    labs(title = 'control interaction log(teamsize)')
  
  p5 <- prior_draws %>% ggplot() + 
    geom_density(aes(`b_condition_fctcontrol:year_after_2010`), fill= "steelblue", color = "black", alpha = 0.6) + 
    geom_density(aes(`b_condition_fctcontrol:year_after_2010`), fill="#FC4E07", color="black",alpha=0.6, data = post_draws) + 
    theme_classic() + 
    labs(title = 'control interaction year')
  
  p6 <- prior_draws %>% ggplot() + 
    geom_density(aes(`b_condition_fctcontrol:year_after_2010`), fill= "steelblue", color = "black", alpha = 0.6) + 
    geom_density(aes(`b_condition_fctcontrol:year_after_2010`), fill="#FC4E07", color="black",alpha=0.6, data = post_draws) + 
    theme_classic() + 
    labs(title = 'control interaction year')
  
  plot_grid(p1, p2, p3, p4, p5, p6, ncol = 2)
}

```

## looks pretty good. 

```{r}

plot_update_beta(m_post_weak, m_prior_weak)

```

## regularize very strongly in the control case. 

```{r}

plot_update_beta(m_post_reg, m_prior_reg)

```

## posterior still tries to escape a bit here. 
especially for control (intercept) but also control (teamsize). 

```{r}

plot_update_beta(m_post_inf, m_prior_inf)

```

## now for other parameters 
### sd, cor, shape

```{r}

plot_update_other <- function(m_post, m_prior){
  
  post_draws <- as_draws_df(m_post)
  prior_draws <- as_draws_df(m_prior)
  
  p1 <- prior_draws %>% ggplot() +
    geom_density(aes(sd_id_match__condition_fctexperiment), fill= "steelblue", color = "black", alpha = 0.6) + 
    geom_density(aes(sd_id_match__condition_fctexperiment), fill="#FC4E07", color="black",alpha=0.6, data = post_draws) + 
    theme_classic() + 
    labs(title = 'sd (random effect) for experiment group')
  
  p2 <- prior_draws %>% ggplot() + 
    geom_density(aes(sd_id_match__condition_fctcontrol), fill= "steelblue", color = "black", alpha = 0.6) + 
    geom_density(aes(sd_id_match__condition_fctcontrol), fill="#FC4E07", color="black",alpha=0.6, data = post_draws) + 
    theme_classic() + 
    labs(title = 'sd (random effect) for control group')
  
  p3 <- prior_draws %>% ggplot() + 
    geom_density(aes(cor_id_match__condition_fctexperiment__condition_fctcontrol), fill= "steelblue", color = "black", alpha = 0.6) + 
    geom_density(aes(cor_id_match__condition_fctexperiment__condition_fctcontrol), fill="#FC4E07", color="black",alpha=0.6, data = post_draws) + 
    theme_classic() + 
    labs(title = 'correlation of random effects?')
  
  p4 <- prior_draws %>% ggplot() + 
    geom_density(aes(shape), fill= "steelblue", color = "black", alpha = 0.6) + 
    geom_density(aes(shape), fill="#FC4E07", color="black",alpha=0.6, data = post_draws) + 
    theme_classic() + 
    labs(title = 'overdispersion for negative binomial')
  
  plot_grid(p1, p2, p3, p4, ncol = 2)
  
}

```

sd looks fine. 
overdispersion looks crazy.

```{r}

plot_update_other(m_post_weak, m_prior_weak)

```

overdispersion (still) crazy.
perhaps too much information in LKJ(5)

```{r}
plot_update_other(m_post_reg, m_prior_reg)
```

looks more reasonable with the new prior for shape. 

```{r}
plot_update_other(m_post_inf, m_prior_inf)
```

# What does theta (shape) parameter (& priors) do? 
Shows that the baseline gamma(0.01, 0.01) is a very weak prior.
https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/monsters-and-mixtures.html

```{r}
install.packages(c("coda","mvtnorm","devtools","loo"))
library(devtools)
devtools::install_github("rmcelreath/rethinking")
```

```{r}

theta = 0.74 # our theta - higher --> more overdispersion. 
mu = 2.16 # intercept for experiment 

post <- ggplot(data = tibble(x = seq(from = 0, to = 10, by = .01)),
       aes(x = x)) +
  geom_ribbon(aes(ymin = 0, 
                  ymax = rethinking::dgamma2(x, mu, theta)),
              color = "transparent", 
              fill = "green") + 
  labs(title = 'actually observed (from model x)')

prior <- ggplot(data = tibble(x = seq(from = 0, to = 30, by = .1)),
       aes(x = x)) +
  geom_ribbon(aes(ymin = 0, 
                  ymax = dgamma(x, 0.01, 0.01)),
              color = "transparent", 
              fill = "green") + 
  labs(title = 'brms default')

exp <- ggplot(data = tibble(x = seq(from = 0, to = 10, by = .1)),
              aes(x = x)) + 
  geom_ribbon(aes(ymin = 0,
                  ymax = dexp(x, rate = 1)),
              color = "transparent",
              fill = "green") + 
  labs(title = 'our new prior')

plot_grid(post, prior, exp)

```


# further posterior sampling checks
https://mc-stan.org/bayesplot/articles/graphical-ppcs.html

```{r}

y <- d$c_5
y_weak <- posterior_predict(m_post_weak, draws = 500)
y_reg <- posterior_predict(m_post_reg, draws = 500)
y_inf <- posterior_predict(m_post_inf, draws = 500)

```

## weak prior
looks like it produces too many zeros in the dens_overlay, and too few in the histogram.

```{r}

ppc_dens_overlay(y, y_weak[1:50, ]) + xlim(0, 50)
ppc_hist(y, y_weak[1:5, ], binwidth = 1) + xlim(-1, 50)
ppc_hist(y, y_weak[1:5, ], binwidth = 100) + xlim(100, 2000)

```

### regularizing prior
better fit for density_overlay()

```{r}

ppc_dens_overlay(y, y_reg[1:50, ]) + xlim(0, 50)
ppc_hist(y, y_reg[1:5, ], binwidth = 1) + xlim(-1, 50)
ppc_hist(y, y_reg[1:5, ], binwidth = 100) + xlim(100, 2000)

```

### informed prior 
also better fit for density_overlay()

```{r}

ppc_dens_overlay(y, y_inf[1:50, ]) + xlim(0, 50)
ppc_hist(y, y_inf[1:5, ], binwidth = 1) + xlim(-1, 50)
ppc_hist(y, y_inf[1:5, ], binwidth = 100) + xlim(100, 2000)

```

## ability of models to produce correct number of zeros  
All models predict too many zeros. 
correct mean is not center of our predictive. 

```{r}

prop_zero <- function(x) mean(x == 0)
ppc_stat(y, y_weak, stat = "prop_zero", binwidth = 0.005)
ppc_stat(y, y_reg, stat = "prop_zero", binwidth = 0.005)
ppc_stat(y, y_inf, stat = "prop_zero", binwidth = 0.005)

```

## ability of models to produce correct maximum 
Looks pretty good. 

```{r}
ppc_stat(y, y_weak, stat = "max")
ppc_stat(y, y_reg, stat = "max")
ppc_stat(y, y_inf, stat = "max")
```

# hypothesis testing 
## Wallentin: when you have an interaction you through out the main effects? 
## --> I am not ready to throw out the main effect - this is what I am mainly interested in. 

```{r}

# main effect
h_intercept <- hypothesis(m_post_inf, 
                          "condition_fctexperiment < condition_fctcontrol",
                          alpha = 0.05) 

# log(teamsize) interaction
h_teamsize <- hypothesis(m_post_inf, 
                         "condition_fctexperiment:log_teamsize < condition_fctcontrol:log_teamsize",
                         alpha = 0.05)

# year after 2010 interaction
h_year <- hypothesis(m_post_inf, 
                     "condition_fctexperiment:year_after_2010 < condition_fctcontrol:year_after_2010",
                     alpha = 0.05)

```

## intercept hypothesis (main effect)

```{r}
# significant main effect (condition experiment > condition control)
# Evid.Ratio: Inf. (or 0, depends on how you frame it)
# Estimate: 0.88 --> exp(0.88) = 2.4 --> citations on average?
h_intercept
plot(h_intercept)

```

```{r}

# significant effect of log(teamsize) 
# Evid.Ratio: 3999
# Estimate: -0.38 --> exp(exp(-0.38)) --> 1.98 --> 1.98 MORE citations per extra author for control on average? 
h_teamsize
plot(h_teamsize)

```

```{r}

# not significant
# slight drift towards replications better over time. 
h_year
plot(h_year)

```

# check estimates

```{r}

# why are the intercepts this low if it should be for mean? 
# i. 1.14 for control is too low it seems (given the mean we found earlier). 
# is it for log_teamsize = 0?
print(m_post_weak)

```

```{r}
# probably pushes the posterior too much (i.e. for control intercept)
# (many) more effective samples in general 
# still very few effective samples for sd() and cor() i.e. group-level effects. 
print(m_post_reg)

```

```{r}
# not sure actually. 
print(m_post_inf)
```

# conditional effects (population estimate)

```{r}

# very certain of the population effect, and then some crazy outliers in both cases. 
# now without points to better see. 
plot(conditional_effects(m_post_inf)) # exp(2.15) = 8.5, but here experiment = ~13. 

```

# model prediction (population estimate + sigma)


```{r}

plot(conditional_effects(m_post_inf, method = 'predict'))

```

# plot model fit (focus on informed)

```{r}
d %>%
  group_by(condition_fct) %>%
  data_grid(log_teamsize = seq_range(log_teamsize, n = 10),
            id_match = id_match,
            year_after_2010 = seq_range(year_after_2010, n = 5)) %>%
              glimpse()
```


https://mjskay.github.io/tidybayes/reference/add_predicted_draws.html

## fitted draws 
Clear that there is a lot of weight for two large studies, 
where the "control" is not cited & the experiment is cited a lot. 
Really seems to drive the (probably spurious) effect. 

```{r}

# with draws 
d %>%
  group_by(condition_fct) %>%
  data_grid(log_teamsize = seq_range(log_teamsize, n = 101),
            #id_match = id_match,
            year_after_2010 = mean(year_after_2010)) %>% # fixed year
  add_epred_draws(m_post_inf, ndraws = 100, re_formula = NA) %>% # ignore random effects
  ggplot(aes(x = log_teamsize, y = c_5, color= ordered(condition_fct))) +
  geom_line(aes(y = .epred, group = paste(condition_fct, .draw)), alpha = 0.25) +
  geom_point(data = d) + 
  labs(title = "fitted draws (population estimate) for fixed year (mean) & ignore random effects")

```

# predict unseen data (create synthetic data)
## Questions:  
### (1) new data (how to get data for e.g. log-teamsize and year_after_2010 that makes sense?)
### i.e. can we just use the same data or do we also sample here?

```{r}

# new ids 
#d_pred <- d %>% group_by(condition_fct) %>%
#  data_grid(id_match = as_factor(rep(1000:2000, each=2)),
#            log_teamsize = log_teamsize, # same log teamsize, should maybe generate synthetic?
#            year_after_2010 = mean(year_after_2010)) %>%
#  add_predicted_draws(m_post_inf, ndraws = 1, allow_new_levels = T)

```

The below works better, but completely re-uses the same values of log(teamsize) and year_after_2010 as in data.
Just assigns new id_matches

```{r}

d_pred <- d %>% select(condition_fct, log_teamsize, year_after_2010) %>%
  mutate(id_match = as_factor(rep(1000:1782, each = 2))) %>% # new ids 
  add_predicted_draws(m_post_inf, ndraws = 100, allow_new_levels = T)
  
```


raw distributions

```{r}

# perhaps freqpoly() instead of histogram()
d_pred %>% ggplot(aes(x = .prediction, color = ordered(condition_fct))) + 
  geom_freqpoly(binwidth = 1, alpha = 0.5, position = 'identity') + 
  geom_vline(xintercept = 100) +
  xlim(0, 200)

```

log-log: 
should probably aggregate the data somehow: 
e.g. power binning. 

```{r}

# adding very small number to avoid log(0). 
p_full <- d_pred %>% group_by(condition_fct, .prediction) %>% 
  summarize(count = n()) %>%
  mutate(log_prediction = log(.prediction + 1),
         log_count = log(count + 1)) %>%
  ggplot(aes(x = log_prediction, y = log_count, color = condition_fct)) + 
  #geom_histogram(binwidth = 1, stat='identity')
  geom_freqpoly(stat="identity") + 
  geom_vline(xintercept = log(100)) +
  labs(title = "log-log plot predicted (simulated) data") 
  
p_sub <- d_pred %>% group_by(condition_fct, .prediction) %>% 
  summarize(count = n()) %>%
  mutate(log_prediction = log(.prediction + 1),
         log_count = log(count + 1)) %>%
  ggplot(aes(x = log_prediction, y = log_count, color = condition_fct)) + 
  #geom_histogram(binwidth = 1, stat='identity')
  geom_freqpoly(stat="identity") + 
  geom_vline(xintercept = log(100)) +
  labs(title = "log-log plot predicted (simulated) data") +
  xlim(0, log(200))

plot_grid(p_full, p_sub)

```

# summary stats (probability of big hit)
can we do this is a more proper way? 
not sure actually how we do it best. 

```{r}

# larger probability of big hit in experiment 
d_pred %>% group_by(condition_fct) %>%
  filter(.prediction > 100) %>%
  summarize(count = n()) %>%
  mutate(fraction = count / sum(count))

# larger probability of above 0 in experiment
d_pred %>% group_by(condition_fct) %>%
  filter(.prediction > 0) %>%
  summarize(count = n()) %>%
  mutate(fraction = count / sum(count))

```

# model comparison (but should be between sensible models)

```{r}

loo_compare(m_post_weak,
            m_post_reg,
            m_post_inf)

loo_model_weights(m_post_weak,
                  m_post_reg,
                  m_post_inf)

```


# probability of superiority? 

```{r}

# code this 

```

# partial pooling 

```{r}

# nice plot of this 

```

# pareto k? 

```{r}

# perhaps investigate?

```

# student t?

```{r}

# robust regression?  

```

